---
title: "Finite precision arithmetic <br> & &nbsp;Intro to optimization"
author: "Aki Nishimura"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "extra.css"]
    lib_dir: libs
    nature:
      highlightStyle: googlecode
      slideNumberFormat: "%current%"
      countIncrementalSlides: false
      beforeInit: "macros.js"
    includes:
      in_header: mathjax_config.html
---

exclude: true

```{r setup, include = FALSE}
# Print outputs without "##"
knitr::opts_chunk$set(comment = '')
```

---

layout: true

# Floating point numbers

Example: 64-bits double-precision number

![](figure/double_precision_number.png)
![:vspace -1ex]

---

52-bits fraction:<br>
$\qquad$ 10110... = 1/2 + 0/4 + 1/8 + 1/16 + 0/32 + ...

--

```{r, eval=TRUE}
c((1 + 2^(-52)) - 1, 
  (1 + 2^(-53)) - 1)
```

---

11-bits exponent:<br>
$\qquad$ $2^{11} = 2048$, covering âˆ’1022 to +1023
![:vspace -.75ex]

--

```{r, eval=TRUE}
c(2^(1024), 2^(1023))
c(2^(-1023 - 52), 2^(-1022 - 52))
```

<!--
```{r, eval=TRUE}
c(2 * 2^(1023), (2 - 2^(-52)) * 2^(1023))
c(2^(-1022 - 52) / 2, 2^(-1022 - 52) / (2 - 2^(-52)))
```
This example is a bit of a surprise b/c `2^(-1022 - 52)` only has 1-binary digit of accuracy, so `2^(-1022 - 52) / (2 - 2^(-52)) == 2^(-1022 - 52)`.
-->

---
layout: false

# Floating point numbers

Example: 64-bits double-precision number

```{r}
.Machine$double.eps # == 2^-52 on most computers
```

```{r}
a <- 1e6 * rnorm(1)
a * (1 + .Machine$double.eps) == a
a * (1 + .Machine$double.eps / 2) == a
```

---

# Floating point numbers

Example: 64-bits double-precision number


```{r, eval=TRUE}
.Machine$double.xmax # == (2 - 2^-52) * 2^1023 on most computers
.Machine$double.xmax * (1 + .Machine$double.eps)
```

--

```{r, eval=TRUE}
log(.Machine$double.xmax)
c(exp(709), exp(710))
```

---
layout: true

# Floating point numbers

### Example: multinomial logit in double-precision

```{r, include=FALSE}
set.seed(1918)
```

```{r}
n_obs <- 5 # just to save memory
n_pred <- 10^6
n_category <- 3

X <- matrix(
  rnorm(n_obs * n_pred), 
  n_obs, n_pred
)
reg_coef <- matrix(
  rnorm(n_pred * n_category), 
  n_pred, n_category
)

category_logp <- as.vector(X[1, ] %*% reg_coef)
category_prob <- exp(category_logp) / sum(exp(category_logp))
```

---

![:vspace -3.1ex]
```{r}
category_prob
```

---

![:vspace -3.1ex]
```{r}
category_logp
```

---
layout:false

# Floating point numbers

### Example: multinomial logit in double-precision

How to deal with the numerical overflow? Note that

$$\frac{\exp(z_\ell)}{\sum_k \exp(z_k)} = \frac{\exp(z_{\ell} - a)}{\sum_k \exp(z_k - a)}$$
for any $a \in \mathbb{R}$. 

--

So the following does the right thing:
![:vspace -1ex]
```{r}
category_logp <- category_logp - max(category_logp)
category_prob <- exp(category_logp) / sum(exp(category_logp))
category_prob
```

---

# Floating point numbers

### Example: multinomial logit in double-precision

More generally, most stats computations can be done in log-likelihood scales, where over/underflow is less likely.

--

Even when you need quantities in the original scale, often it is the ratio of likelihoods/probabilities that matters, e.g.

--

* Cluster assignment probability calculation in the EM algorithm for Gaussian mixture models. 
* Acceptance probability calculation in Metropolis-Hastings algorithm.

---
layout: true

# Floating point numbers

### Example: accuracy of numerical gradient

---

Numerical differentiation is one practical situation in which you clearly observe an effect of finite-precision.

--

Taylor's theorem tells us that
$$f(x + \Delta x) = f(x) + \Delta x f'(x) + \frac{(\Delta x)^2}{2} f''(x) + \ldots.$$

--

So we can approximate $f'(x)$ using the relation
$$\frac{f(x + \Delta x) - f(x)}{\Delta x} \approx f'(x) + O(\Delta x).$$
---

But we can do better; note that
.small[$$f(x + \Delta x) = f(x) + \Delta x f'(x) + \frac{(\Delta x)^2}{2} f''(x) + \frac{(\Delta x)^3}{6} f'''(x) + \ldots$$]
.small[$$f(x - \Delta x) = f(x) - \Delta x f'(x) + \frac{(\Delta x)^2}{2} f''(x) - \frac{(\Delta x)^3}{6} f'''(x) + \ldots$$]

--

So we have 
$$\frac{f(x + \Delta x) - f(x - \Delta x)}{2 \Delta x} \approx f'(x) + O((\Delta x)^2),$$
which is called _centered difference approximation_.

---

We can extend the idea to get higher-order estimates e.g.
.small[$$\frac{- f(x + 2 \Delta x) + 8 f(x + \Delta x) - 8 f(x - \Delta x) + f(x - 2 \Delta x)
  }{12 \Delta x} 
  = f'(x) + O((\Delta x)^4).$$]

--

BUT these methods aren't necessarily better in practice.
(You will find out why that might be in the homework.)

--

**Note:** For functions with multivariate inputs, we have 
$f(\boldsymbol{x} + \Delta x \, \boldsymbol{e}_i) = f(\boldsymbol{x}) + \Delta x \, \partial_i f(\boldsymbol{x}) + \ldots$.
