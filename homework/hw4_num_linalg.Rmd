---
title: 'Homework: Numerical linear algebra'
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: none
    highlight: textmate
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      bA: "{\\boldsymbol{A}}",
      bx: "{\\boldsymbol{x}}",
      bb: "{\\boldsymbol{b}}"
    }
  }
});
</script>


```{r setup, include=FALSE}
source(file.path("..", "R", "util.R"))

required_packages <- c("microbenchmark", "bench")
install_and_load_packages(required_packages)
```


# Exercise 1: Comparing different numerical linear algebra algorithms for solving linear systems

In this exercise, we consider the problem of solving a linear system $\bA \bx = \bb$ for $\bx$.
We compare the three methods we learned in the class: LU, Cholesky, and QR decompositions.
(Of course, LU applies to more general systems and QR can be used to solve least squares, but here we focus on positive definite systems.)

## Part A: Racing for solution &mdash; speed comparison 

We first compare their computational speed. 
Fill in the code below and `bench::mark()` the three algorithms.

**Questions:**
What are relative speeds among the algorithms?
Do relative speeds agree with what you expect from the complexity analyses?
If not (quite), why might that be?

**Response:**
Here, the Cholesky decomposition is fastest with median time 379ms, followed by the LU decomposition with median time 458ms, and then finally QR decomposition, the slowest with median time 1.13s. This ordering makes some sense given the complexity analyses undergone for these algorithms during lecture, where it was indicated that Cholesky decomposition has complexity $\sim \frac{1}{3}p^3$, LU decomposition has complexity $\sim \frac{2}{3}p^3$, and QR decomposition has $\sim 2np^2 - \frac{2}{3}p^3 = \frac{4}{3}p^3$ for this $n=p$ case with $p$ parameters and $n$ number of data points. Note that, while the QR-decomposition complexity analysis performed in class was for solving least squares, this is actually algebraically identical to solving the system, as solving $Ax = b$ for $x$ and $A = QR$ implies calculating $R^{-1}Q^Tb$, the same formulation used to find coefficients in least squares. This means that the Cholesky decomposition should take about half of the time of the LU decomposition, which itself would take around half the time of the QR decomposition. However, these relationships do not play out exactly here. According to the tests performed, the Cholesky decomposition-based method is only marginally faster than LU decomposition, with the reduction in runtime being only 20% rather than the expected 50%. As for the relative relationship between the LU decomposition and the QR decomposition, the test here provided an even greater difference than expected. The LU decomposition took even less than half of the time of the QR decomposition. These deviations from the theoretical expectations are likely due to different optimizations under the hood for each of these methods, such that the full decompositions are likely not being calculated for the LU and Cholesky decompositions.


**Note:**
I misspoke about the R's built-in `chol()` function during the lecture:
when applied to a positive-definite `A`, the function actually returns an _upper-triangular_ matrix `R` such that `t(R) %*% R == A`.

```{r, eval=FALSE}
# Import the `rand_positive_def_matrix` function
source(file.path("R", "random_matrix.R"))

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
b <- rnorm(mat_size)

solve_via_cholesky <- function(A, b) {
  R = chol(A)
  return(backsolve(R, forwardsolve(t(R), b)))
}

bench::mark(
  solve(A, b), min_iterations = 100
)
bench::mark(
  solve_via_cholesky(A, b), min_iterations = 100
)
bench::mark(
  qr.solve(A, b), min_iterations = 100
)
```

## Part B: Competition in terms of numerical accuracy/stability

We now compare the three methods in terms of numerical accuracy/stability.
To this end, we set up the following simulation study. 
We first generate a "ground truth" solution vector $\bx_0$.
We then compute an "numerical-error-free" $\bb = \bA \bx_0$ by carrying out the matrix-vector multiplication using the `long double` type, which (on most hardware and compilers) provides [additional 12 bits of precision](https://en.wikipedia.org/wiki/Extended_precision#x86_extended_precision_format).
Of course, the vector $\bb$ computed as such still suffers from numerical errors, but the idea is that the numerical errors from this high-precision matrix-vector multiplication is much smaller than the errors caused by numerically solving $\bA \bx = \bb$ for $\bx$.
We can thus assess the accuracy of the three solvers by comparing the numerically computed $\bx$ to the ground truth $\bx_0$.

### Task &#x2F00;

First compare the outputs of matrix-vector multiplication $\bx \to \bA \bx$ using `double` and `long double` using the provided Rcpp functions.

**Questions:**
What is the relative difference in $\ell^2$-norm? 
How about the coordinate-wise relative differences?
Are the observed magnitudes of the differences what you'd expect?

**Response:** 
The relative difference in $\ell^2$-norm due to moving from double precision to long double is -3.958355e-16, while the coordinate-wise relative differences have mean -2.877685e-16, variance 4.761493e-28, and quantiles described by the following table:

```{r, echo=FALSE, warning=FALSE}
source(file.path("R", "random_matrix.R"))
Rcpp::sourceCpp(file.path("src", "matvec_double.cpp"))
Rcpp::sourceCpp(file.path("src", "matvec_ldouble.cpp"))

set.seed(1918)

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
x <- rnorm(mat_size)

std_result = matvec_double(A, x)
prec_result = matvec_ldouble(A, x)

std_l2 = sqrt(sum((std_result)^2))
prec_l2 = sqrt(sum((prec_result)^2))

relative_l2 = (prec_l2 - std_l2)/prec_l2

relative_coord = (prec_result - std_result)/prec_result
quantile(relative_coord, p = c(0, 0.25, 0.5, 0.75, 1))
```

To see whether these are logical magnitudes of difference, recall first the result from lecture indicating that algorithms for solving $Ax = b$ with $b$ known should satisfy $\frac{||x_{true} - x||}{||x||} = \mathcal{O}(\kappa(A)\epsilon_{machine})$. This implies then that the same thing holds for solving $x = A^{-1}b$ for known $b$. Given this, as well as the fact that $\kappa(A) = \frac{1}{\kappa(A^{-1})}$ by definition, it follows that the problem $v \rightarrow Av$ with output vector $x$ will have relative error satisfying $\frac{||x_{true} - x||}{||x||} = \mathcal{O}(\frac{\epsilon_{machine}}{\kappa(A)})$. 

Now, for matrix $A$ as generated, the condition number is known to be $1e3$. Then, the machine precisions for the double and long double case are $2^{-52} \approx 2.220446e-16$ and $2^{-63} \approx 1.084202e-19$ respectively. These facts, in conjunction with the condition number for $A$ as chosen, imply then that the relative errors in the outputs of the two matrix multiplications (from the actual ground truth) should be on the order of $1e-16$ and $1e-19$ for the double and long-double case respectively. One can then apply the triangle inequality and approximation of the exact output to find that the relative difference in $\ell^2$-norm will be around the order $1e-16$, which indeed holds in this case. So yes, the magnitude of relative difference in $\ell-2$ is as expected. 

The element-wise differences are also not entirely unexpected, as in the worst case one could easily see orders on the scale well above the order of $2^{-52}$ in each entry of $b$ simply due to accumulated errors stemming from the difference in machine precision and compounding due to matrix-vector multiplication of $2^{10} = 1024$-dimensional inputs. This accounts for the large distribution in errors presented above. However, assuming something like uniformity of the missing digits of precision, one would expect on average for the error to be around the double machine precision, and this is what is observed. 


```{r, eval=FALSE}
Rcpp::sourceCpp(file.path("src", "matvec_double.cpp"))
Rcpp::sourceCpp(file.path("src", "matvec_ldouble.cpp"))

set.seed(1918)

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
x <- rnorm(mat_size)

std_result = matvec_double(A, x)
prec_result = matvec_ldouble(A, x)

std_l2 = sqrt(sum((std_result)^2))
prec_l2 = sqrt(sum((prec_result)^2))

relative_l2 = (prec_l2 - std_l2)/prec_l2

relative_coord = (prec_result - std_result)/prec_result
mean(relative_coord)
var(relative_coord)
quantile(relative_coord, p = c(0, 0.25, 0.5, 0.75, 1))
```

### Task &#x2F06;

Now randomly generate $\bA$ so that its condition number is $10^6$.
Then solve a positive-definite system $\bA \bx = \bb$ for $\bx$ using the three algorithms and compare their outputs to the ground truth $\bx_0$.

**Questions:**
Which algorithm appears to be more accurate than the others? 
Visually demonstrate your answer.

```{r}
set.seed(1918)
cond_num <- 1e6

# Larger matrices could incur substantial computational time under base R BLAS
mat_size <- 1024L 

A <- rand_positive_def_matrix(mat_size, cond_num)
x <- rnorm(mat_size)
x_l2 = sqrt(sum(x^2))
  
# Solve with higher precision
b <- matvec_ldouble(A, x)

# LU Decomposition
LU_soln = solve(A, b)
LU_soln_l2 = sqrt(sum((LU_soln)^2))
print((LU_soln_l2 - x_l2)/x_l2)

# Cholesky Decomposition
R = chol(A)
Chol_soln = backsolve(R, forwardsolve(t(R), b))
Chol_soln_l2 = sqrt(sum((Chol_soln)^2))
print((Chol_soln_l2 - x_l2)/x_l2)

# QR Decomposition
QR_soln = qr.solve(A, b)
QR_soln_l2 = sqrt(sum((QR_soln)^2))
print((QR_soln_l2 - x_l2)/x_l2)
```

```{r, echo = FALSE, cache = TRUE}
library(ggplot2)
library(tidyverse)
plot_df = data.frame(Method = c("LU", "LU", "Cholesky", "Cholesky", "QR", "QR"),
                     Difference = c((LU_soln_l2 - x_l2)/x_l2, sqrt(sum((LU_soln - x)^2)),
                                     (Chol_soln_l2 - x_l2)/x_l2, sqrt(sum((Chol_soln - x)^2)),
                                     (QR_soln_l2 - x_l2)/x_l2, sqrt(sum((QR_soln - x)^2))),
                     Metric = c("Relative L2", "Euclidean", "Relative L2", "Euclidean", 
                                "Relative L2", "Euclidean"))

plot_df %>%
  ggplot(aes(fill = Method, y = Difference, x = Method)) + 
  geom_bar(position="dodge", stat="identity") + 
  facet_wrap(~Metric, ncol = 1, scales = "free_y")
```

First, bar-charts were used to visualize the error, summarized using both Euclidean distance and Relative L2-norm difference. This provides a great visualization of the relative magnitudes of the errors, but not really how the results are situated relative in space. For this, refer to the next plot.

```{r, echo=FALSE, cache=TRUE}
library(ggplot2)
library(tidyverse)
plot_df = data.frame(X = x, LU_x = LU_soln, Chol_x = Chol_soln, QR_x = QR_soln) %>%
  as.matrix() %>%
  t()

mds = cmdscale(dist(plot_df), k = 2) %>%
  data.frame() %>%
  rownames_to_column(var = "label") %>%
  mutate(dist = sqrt((X1 - first(X1))^2 + (X2 - first(X2))^2)) %>%
  mutate(dist = formatC(dist, format = "e", digits = 2)) %>%
  mutate(dist = case_when(dist == "0.00e+00" ~ "",
                          TRUE ~ dist)) %>%
  mutate(Y1 = first(X1), Y2 = first(X2))

ggplot(mds, aes(x = X1, y = X2, label = label)) + 
  geom_point() + 
  geom_text(nudge_y = 0, nudge_x = -2.5e-11) + 
  geom_text(aes(label = dist), nudge_y = 0, nudge_x = 3e-11) + 
  geom_segment(aes(x = X1, y = X2, xend = Y1, yend = Y2)) + 
  xlim(-3e-10, 2e-10)
```

Note that the above plot is a multi-dimensional scaling of the $x$ vectors produced using each solving procedure. This plot projects the $x$ points in 1024-dimensional space down to 2-dimensions while preserving the distances between them. Do note that the y-axis is much finer than the x-axis here, so the best reference for the actual distances is the previous plot, or the labels attached to each solution point in the scaled result. The plot itself serves mostly for spatial orientation. 

According to the above two plots, the LU decomposition is the closest to the "ground truth" x vector, followed by the Cholesky decomposition, then finally the QR decomposition, and the answers all appear to have spaced around the ground truth itself, without a clear preference for a certain direction of error (at least, nothing can be said based upon this very small sample).

### Task &#x4E09;

In Task &#x2F06;, we compared the three algorithms in one randomly generated example.
Now we consider a more systematic (though hardly comprehensive) comparison via repeated simulations.
We also vary the condition number of $\bA$ and assess whether the results would hold across varying degrees of ill-conditioning.

**Questions/To-do's:**

* Using the starter code provided, calculate various summary measures of the numerical errors.
* Integrate into the provided code one another (or more, if you like) meaningful metric(s) of your choice to summarize the numerical error.
* Visually explore how the three algorithms compare with each other in their accuracy. See if different error metrics tell different stories; they might or might not.
* Vary the condition number in the range $10^6 \sim 10^{12}$, e.g. by trying $10^6$, $10^9$, and $10^{12}$.
* Do you see any patterns in the numerical errors across the three algorithms, metrics, and/or condition numbers? Show some plots to support your conclusion.

**Note:** 
The QR solver will throw an error when the system is ill-condition enough that the numerical solution might not be very accurate. 
To force it to return the solution in any case, set `tol = .Machine$double.eps`.

```{r}
# Utility functions for bookkeeping simulation results.
library(tidyverse)
source(file.path("R", "num_linalg_sim_study_helper.R"))

perform_simulation <- function(cond_num = 1e6) {
  n_sim <- 250L
  mat_size <- 512L
  metrics <- c("norm", "median", "percentile5", "percentile95",
               "l1norm")
  
  rel_error_list <- lapply(c("lu", "chol", "qr"),
                           function(method)
                             pre_allocate_error_list(n_sim, metrics))
  
  for (sim_index in 1:n_sim) {
    A <- rand_positive_def_matrix(mat_size, cond_num)
    x <- rnorm(mat_size)
    b <- matvec_ldouble(A, x)
    
    R = chol(A)
    x_approx <- list(
      lu = solve(A, b),
      chol = backsolve(R, forwardsolve(t(R), b)),
      qr = qr.solve(A, b, tol = .Machine$double.eps)
    )
    for (method in c("lu", "chol", "qr")) {
      rel_error <- lapply(metrics,
                          function (metric)
                            calc_rel_error(x, x_approx[[method]], metric))
      names(rel_error) <- metrics
      for (metric in names(rel_error)) {
        rel_error_list[[method]][[metric]][sim_index] <- rel_error[[metric]]
      }
    }
  }
  
  qr_df <-
    data.frame(Method = "QR", errors = rel_error_list[["qr"]]) %>%
    pivot_longer(!Method, names_to = "Metric", values_to = "Error") %>%
    separate(Metric, c(NA, "Metric"))
  
  lu_df <-
    data.frame(Method = "LU", errors = rel_error_list[["lu"]]) %>%
    pivot_longer(!Method, names_to = "Metric", values_to = "Error") %>%
    separate(Metric, c(NA, "Metric"))
  
  ch_df <-
    data.frame(Method = "Chol", errors = rel_error_list[["chol"]]) %>%
    pivot_longer(!Method, names_to = "Metric", values_to = "Error") %>%
    separate(Metric, c(NA, "Metric"))
  
  plot_df = rbind(rbind(qr_df, lu_df), ch_df)
  out_plot = plot_df %>%
    ggplot(aes(x = Error)) + 
    geom_density(aes(group = Method, color = Method, fill = Method), alpha = 0.2) + 
    facet_wrap(~ Metric, scales = "free") + 
    scale_y_continuous(breaks = c()) + 
    theme(axis.text.x = element_text(angle = 45, vjust = 0, hjust=0.1))
  
  return(out_plot)
}
```

Note that the visual exploration of the different error metrics for each solving method was facilitated by functionalization of simulation running and plot production. In this procedure, the distributions of each error measure for each solving method are calculated based upon the simulated data and displayed. Here, as the relative shapes of the distribution between the 3 solving methods is the most important feature, the actual density magnitudes are not displayed.

## Condition number 1e6 Visualizations
```{r, cache=TRUE}
perform_simulation(1e6)
```

## Condition number 1e9 Visualizations
```{r, cache=TRUE}
perform_simulation(1e9)
```

## Condition number 1e12 Visualizations
```{r, cache=TRUE}
perform_simulation(1e12)
```

**Elaboration upon Results:** 
The most striking pattern above, which appears to be the case for all 5 metrics which summarize the calculation error, is that the QR decomposition has a much wider error distribution compared to those of the Cholesky and LU decompositions, which are comparable to one another in most cases with Cholesky often having perhaps a slightly higher density closer to zero. As such, it is clear from these simulations and the corresponding figures that the QR decomposition tends to produce greater errors across all 3 condition numbers than the other two decompositions. Returning to the stability results from lecture, recall that using the Cholesky and LU decompositions should result in normed error of order equal to the product of the matrix condition number and machine error, $O(\kappa(A)\epsilon_{machine})$, while the QR decomposition applied to this task (using Householder matrices) achieves $[\kappa(A) + \frac{||Ax_{machine} - y||}{||Ax_{machine}||}\kappa^2(A)] \times \epsilon_{machine}$, which will inherently be larger, making the results above logical. This is where it becomes evident that, while the QR decomposition saves quite a lot of work and is quite stable for the task of solving least squares, it does not receive those same gains for solving simple systems. It performs the same procedure here as in solving least squares, causing it to be less stable and take longer than the other algorithms.
