---
title: 'Homework: Numerical linear algebra'
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: none
    highlight: textmate
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      bA: "{\\boldsymbol{A}}",
      bx: "{\\boldsymbol{x}}",
      bb: "{\\boldsymbol{b}}"
    }
  }
});
</script>


```{r setup, include=FALSE}
source(file.path("..", "R", "util.R"))

required_packages <- c("microbenchmark", "bench")
install_and_load_packages(required_packages)
```


# Exercise 1: Comparing different numerical linear algebra algorithms for solving linear systems

In this exercise, we consider the problem of solving a linear system $\bA \bx = \bb$ for $\bx$.
We compare the three methods we learned in the class: LU, Cholesky, and QR decompositions.
(Of course, LU applies to more general systems and QR can be used to solve least squares, but here we focus on positive definite systems.)

## Part A: Racing for solution &mdash; speed comparison 

We first compare their computational speed. 
Fill in the code below and `bench::mark()` the three algorithms.

**Questions:**
What are relative speeds among the algorithms?
Do relative speeds agree with what you expect from the complexity analyses?
If not (quite), why might that be?

**Response:**
Here, the Cholesky decomposition is fastest with median time 233ms, followed by the LU decomposition with median time 453ms, and then finally QR decomposition, the slowest with median time 834ms. This ordering makes some sense given the complexity analyses undergone for these algorithms during lecture, where it was indicated that Cholesky decomposition has complexity of order $\frac{1}{3}p^3$, LU decomposition has complexity of order $\frac{2}{3}p^3$, and QR decomposition has order $2np^2$ for $p$ parameters and $n$ number of data points. Here, $n = p = 1024$, meaning that the Cholesky decomposition should take about half of the time of the LU decomposition, which itself would take around $1/3$ the time of the QR decomposition. The relative timing relationship between the Cholesky and LU decompositions actually plays out nearly exactly as expected, but the QR decomposition-based solving does not take 3 times as long as the LU decomposition; the time taken is in fact less than doubled. This is likely due to optimization under the hood for the QR decomposition, such that the full decomposition is likely not being calculated - evidenced by the fact that the QR matrices are not accessible/returned through the qr() family of functions. For the LU decomposition on the other hand, it is likely that the full decomposition is determined, as the standard procedure for solving via LU decomposition inherently involves explicit calculation of L and U.


**Note:**
I misspoke about the R's built-in `chol()` function during the lecture:
when applied to a positive-definite `A`, the function actually returns an _upper-triangular_ matrix `R` such that `t(R) %*% R == A`.

```{r, eval=FALSE}
# Import the `rand_positive_def_matrix` function
source(file.path("R", "random_matrix.R"))

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
b <- rnorm(mat_size)

solve_via_cholesky <- function(A, b) {
  R = chol(A)
  return(backsolve(R, forwardsolve(t(R), b)))
}

bench::mark(
  solve(A, b)
)
bench::mark(
  solve_via_cholesky(A, b)
)
bench::mark(
  qr.solve(qr(A), b)
)
```

## Part B: Competition in terms of numerical accuracy/stability

We now compare the three methods in terms of numerical accuracy/stability.
To this end, we set up the following simulation study. 
We first generate a "ground truth" solution vector $\bx_0$.
We then compute an "numerical-error-free" $\bb = \bA \bx_0$ by carrying out the matrix-vector multiplication using the `long double` type, which (on most hardware and compilers) provides [additional 12 bits of precision](https://en.wikipedia.org/wiki/Extended_precision#x86_extended_precision_format).
Of course, the vector $\bb$ computed as such still suffers from numerical errors, but the idea is that the numerical errors from this high-precision matrix-vector multiplication is much smaller than the errors caused by numerically solving $\bA \bx = \bb$ for $\bx$.
We can thus assess the accuracy of the three solvers by comparing the numerically computed $\bx$ to the ground truth $\bx_0$.

### Task &#x2F00;

First compare the outputs of matrix-vector multiplication $\bx \to \bA \bx$ using `double` and `long double` using the provided Rcpp functions.

**Questions:**
What is the relative difference in $\ell^2$-norm? 
How about the coordinate-wise relative differences?
Are the observed magnitudes of the differences what you'd expect?

**Response:** 
The relative difference in $\ell^2$-norm due to moving from double precision to long double is -3.958355e-16, while the coordinate-wise relative differences have mean -2.877685e-16, variance 4.761493e-28, and quantiles described by the following table (with the full explicit vector printed as output):

```{r, echo=FALSE}
source(file.path("R", "random_matrix.R"))
Rcpp::sourceCpp(file.path("src", "matvec_double.cpp"))
Rcpp::sourceCpp(file.path("src", "matvec_ldouble.cpp"))

set.seed(1918)

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
x <- rnorm(mat_size)

# Fill in
std_result = matvec_double(A, x)
prec_result = matvec_ldouble(A, x)

std_l2 = sqrt(sum((std_result)^2))
prec_l2 = sqrt(sum((prec_result)^2))

relative_l2 = (prec_l2 - std_l2)/std_l2

relative_coord = (prec_result - std_result)/std_result
quantile(relative_coord, p = c(0, 0.25, 0.5, 0.75, 1))
```

To see whether these are logical magnitudes of difference, recall first the result from lecture indicating that algorithms for solving $Ax = b$ with $b$ known should satisfy $\frac{||x_{true} - x||}{||x||} = \mathcal{O}(\kappa(A)\epsilon_{machine})$. This implies then that the same thing holds for solving $x = A^{-1}b$ for known $b$. Given this, as well as the fact that $\kappa(A) = \frac{1}{\kappa(A^{-1})}$ by definition, it follows that the problem $v \rightarrow Av$ with output $x$ will have relative error satisfying $\frac{||x_{true} - x||}{||x||} = \mathcal{O}(\frac{\epsilon_{machine}}{\kappa(A)})$. 

Now, for matrix $A$ as generated, the condition number is known to be $1e3$. Then, the machine precisions for the double and long double case are $2^{-52} \approx 2.220446e-16$ and $2^{-63} \approx 1.084202e-19$ respectively. These facts, in conjunction with the condition number for $A$ as chosen, imply then that the relative errors in the outputs of the two matrix multiplications should be on the order of $1e-16$ and $1e-19$ for the double and long-double case respectively. One can then apply the triangle inequality and approximation of the exact output to find that the relative difference in $\ell^2$-norm will be $\mathcal{O}(1e-16)$, which plays out exactly in this case, so indeed the magnitude of relative difference in $\ell-2$ is as expected. 

The element-wise differences are also not entirely unexpected, as in the worst case one could easily see orders on the scale well above the order of $2^{-52}$ in each entry of $b$ simply due to accumulated errors stemming from the difference in machine precision and compounding due to matrix-vector multiplication of $2^10 = 1024$-dimensional inputs. This accounts for the large distribution in errors presented above. However, assuming something like uniformity of the missing digits of precision, one would expect on average for the error to be around the double machine precision, and this is what is observed. 


```{r, eval=FALSE}
Rcpp::sourceCpp(file.path("src", "matvec_double.cpp"))
Rcpp::sourceCpp(file.path("src", "matvec_ldouble.cpp"))

set.seed(1918)

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
x <- rnorm(mat_size)

# Fill in
std_result = matvec_double(A, x)
prec_result = matvec_ldouble(A, x)

std_l2 = sqrt(sum((std_result)^2))
prec_l2 = sqrt(sum((prec_result)^2))

relative_l2 = (prec_l2 - std_l2)/std_l2

relative_coord = (prec_result - std_result)/std_result
mean(relative_coord)
var(relative_coord)
```

### Task &#x2F06;

Now randomly generate $\bA$ so that its condition number is $10^6$.
Then solve a positive-definite system $\bA \bx = \bb$ for $\bx$ using the three algorithms and compare their outputs to the ground truth $\bx_0$.

**Questions:**
Which algorithm appears to be more accurate than the others? 
Visually demonstrate your answer.

```{r, results=FALSE}
set.seed(1918)
cond_num <- 1e6

# Larger matrices could incur substantial computational time under base R BLAS
mat_size <- 1024L 

A <- rand_positive_def_matrix(mat_size, cond_num)
x <- rnorm(mat_size)
x_l2 = sqrt(sum(x^2))
  
# Solve with higher precision
b <- matvec_ldouble(A, x)

# LU Decomposition
LU_soln = solve(A, b)
LU_soln_l2 = sqrt(sum((LU_soln)^2))
print((LU_soln_l2 - x_l2)/x_l2)
print(sqrt(sum((LU_soln - x)^2)))

# Cholesky Decomposition
R = chol(A)
Chol_soln = backsolve(R, forwardsolve(t(R), b))
Chol_soln_l2 = sqrt(sum((Chol_soln)^2))
print((Chol_soln_l2 - x_l2)/x_l2)
print(sqrt(sum((Chol_soln - x)^2)))

# QR Decomposition
QR_soln = qr.solve(qr(A), b)
QR_soln_l2 = sqrt(sum((QR_soln)^2))
print((QR_soln_l2 - x_l2)/x_l2)
print(sqrt(sum((QR_soln - x)^2)))
```

```{r, echo=FALSE}
library(ggplot2)
library(tidyverse)
plot_df = data.frame(X = x, LU_x = LU_soln, Chol_x = Chol_soln, QR_x = QR_soln) %>%
  as.matrix() %>%
  t()

mds = cmdscale(dist(plot_df), k = 2) %>%
  data.frame() %>%
  rownames_to_column(var = "label") %>%
  mutate(dist = sqrt((X1 - first(X1))^2 + (X2 - first(X2))^2)) %>%
  mutate(dist = formatC(dist, format = "e", digits = 2)) %>%
  mutate(dist = case_when(dist == "0.00e+00" ~ "",
                          TRUE ~ dist)) %>%
  mutate(Y1 = first(X1), Y2 = first(X2))

ggplot(mds, aes(x = X1, y = X2, label = label)) + 
  geom_point() + 
  geom_text(nudge_y = 0, nudge_x = -2.5e-11) + 
  geom_text(aes(label = dist), nudge_y = 0, nudge_x = 3e-11) + 
  geom_segment(aes(x = X1, y = X2, xend = Y1, yend = Y2)) + 
  xlim(-3e-10, 2e-10)
```

Note that the above plot is a multi-dimensional scaling of the $x$ vectors produced using each solving procedure. This plot projects the $x$ points in 1024-dimensional space down to 2-dimensions while preserving the distances between them. According to this plot, the Cholesky decomposition is the closest to the "ground truth" x vector, followed by LU decomposition, then finally the QR decomposition.

### Task &#x4E09;

In Task &#x2F06;, we compared the three algorithms in one randomly generated example.
Now we consider a more systematic (though hardly comprehensive) comparison via repeated simulations.
We also vary the condition number of $\bA$ and assess whether the results would hold across varying degrees of ill-conditioning.

**Questions/To-do's:**

* Using the starter code provided, calculate various summary measures of the numerical errors.
* Integrate into the provided code one another (or more, if you like) meaningful metric(s) of your choice to summarize the numerical error.
* Visually explore how the three algorithms compare with each other in their accuracy. See if different error metrics tell different stories; they might or might not.
* Vary the condition number in the range $10^6 \sim 10^{12}$, e.g. by trying $10^6$, $10^9$, and $10^{12}$.
* Do you see any patterns in the numerical errors across the three algorithms, metrics, and/or condition numbers? Show some plots to support your conclusion.

**Note:** 
The QR solver will throw an error when the system is ill-condition enough that the numerical solution might not be very accurate. 
To force it to return the solution in any case, set `tol = .Machine$double.eps`.

```{r, eval=FALSE}
# Utility functions for bookkeeping simulation results.
source(file.path("R", "num_linalg_sim_study_helper.R"))

n_sim <- 32L
mat_size <- 512L
cond_num <- 1e6
metrics <- c("norm", "median", "five_percentile", "ninety_five_percentile", 
             "l1norm")

rel_error_list <- lapply(
  c("lu", "chol", "qr"), 
  function(method) pre_allocate_error_list(n_sim, metrics)
)

for (sim_index in 1:n_sim) {
  A <- rand_positive_def_matrix(mat_size, cond_num)
  x <- rnorm(mat_size) 
  b <- matvec_ldouble(A, x)
  
  R = chol(A)
  x_approx <- list( 
    lu = solve(A, b),
    chol = backsolve(R, forwardsolve(t(R), b)),
    qr = qr.solve(qr(A), b, tol = .Machine$double.eps)
  )
  for (method in c("lu", "chol", "qr")) {
    rel_error <- lapply(
      metrics, 
      function (metric) calc_rel_error(x, x_approx[[method]], metric)
    )
    names(rel_error) <- metrics
    for (metric in names(rel_error)) {
      rel_error_list[[method]][[metric]][sim_index] <- rel_error[[metric]]
    }
  }
}

qr_df <- data.frame(Method = "QR", errors = rel_error_list[["qr"]])
lu_df <- data.frame(Method = "LU", errors = rel_error_list[["lu"]])
ch_df <- data.frame(Method = "Chol", errors = rel_error_list[["chol"]])

plot_df = rbind(rbind(qr_df, lu_df), ch_df)


```
