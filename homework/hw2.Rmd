---
title: 'Homework: Finite Precision Arithmetic & Gradient Descent'
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: none
    highlight: textmate
---


```{r setup, include=FALSE}
rm(list = ls())
source(file.path("..", "R", "colors.R"))
source(file.path("..", "R", "util.R"))

required_packages <- c('Matrix', 'testthat')
install_and_load_packages(required_packages)
```


# Exercise 1: One-pass algorithm for calculating variance

Let's say you are running a Monte Carlo simulation to estimate the mean and variance of some random variable $\boldsymbol{\theta}$. 
Imagine that $\boldsymbol{\theta}$ is high-dimensional, so you don't want to keep all the Monte Carlo samples $\boldsymbol{\theta}_i$ due to the memory burden.
(If $\boldsymbol{\theta}$ is $10^6$-dimensional, for example, storing 1,000 Monte Carlo samples would require 8GB of memory.)
And it occurs to you that you can write a "one-pass" algorithm to calculate the sample mean and sample second moment of $\boldsymbol{\theta}$ by only keeping track of running totals of Monte Carlo samples. 
You can then, in theory, calculate the sample variance from these quantities.

The above idea is implemented and applied below. 
Try it and see what you get. 
Does the estimate make sense? If not, why?
How does the estimate change if you change the seed?

**Bonus question:** 
What is, roughly speaking, the distribution of the estimator `sample_var_with_question_mark` under repeated Monte Carlo simulations (i.e. without a fixed seed)?

```{r}
set.seed(140778)

running_sum <- 0
running_sum_sq <- 0

n_sample <- 10^3
for (i in 1:n_sample) {
  theta <- 1e10 + rnorm(1)
  running_sum <- running_sum + theta
  running_sum_sq <- running_sum_sq + theta^2
}
sample_mean <- running_sum / n_sample
sample_sec_mom <- running_sum_sq / n_sample

sample_var_with_question_mark <- (sample_sec_mom - (sample_mean)^2)

sample_mean
sample_sec_mom
sample_var_with_question_mark
```

## Things to commit/submit
Answers to the questions above.

### Outputs from the Script
The sample mean: ~1e10 (9999999999.97963)

The sample second moment: ~1e20 (99999999999592366080)

The supposed sample variance: -147456

### Elaboration
First, the sample mean makes logical sense. The variable being simulated from is normal with mean 1e10 and variance 1, so it should have mean quite close to 1e10, which is in fact the case. 
For the second moment, note that the value, according to the characteristics of the normal distribution, should be equal to the variance plus the mean squared, or 1e20+1. This is actually quite close to what is observed in the simulations as well.

Finally, consider the sample variance. This value is actually negative, which should not be the case for any proposed variance estimate, as the variance is strictly positive for non-deterministic random variables. Not only is it negative, it is extremely negative. This indicates that indeed this estimator is somewhat off. Changing the random seed, one finds that this is not really a problem of this estimate always being negative, rather there is quite large variability in this calculated sample variance. It ranges into the postive hundreds of thousands as well; all values which are quite far from the statistically correct answer of 1. This is likely due to the very large mean of the target distribution and how it affects downstream calculations. Squaring the large values generated for estimation of the second moment, as well as squaring the mean calculated from the running first-moment sum, greatly amplifies the deviations from the actual mean, resulting in much greater variability in the second moment estimate, as well as the squared first moment estimate. These two high variabilities then cause the lack of the estimators getting close to the actual variance of 1. 

### Bonus
First, it is assumed that each $\theta$ draw will be IID normal with mean 1e10 and variance 1. This means that the sample mean, as a linear combination of these normal random draws, will be itself normal. In particular, it will have mean 1e10 and variance 1e-3. Next, it should be recalled that a sum of $k$ IID squared normals with means $\mu$ and variances 1 will have chi-square distribution with noncentrality parameter $\lambda =  k\mu^2$ and degrees of freedom k. Based upon this information, one can first derive that 1e3 multiplied by the sample mean squared will have distribution of chi-square with noncentrality parameter $\lambda$ = 1e26 and degrees of freedom 1. This means that the sample mean squared can be thought of as this chi-square distribution scaled by 1e3. One can next discern that the running sum of squares is also distributed as chi square with noncentrality parameter $\lambda$ = 1e23 and degrees of freedom 1e3. So then, the sample variance estimator is a weighted summation of non-central chi-square distributed random variables, making it distributed as generalized chi-square. 


# Exercise 2: Numerical differentiation

## Part A: Potential caveats of numerical differentiation

You have learned in the class that the centered difference $\frac{f(x + \Delta x) - f(x - \Delta x)}{2 \Delta x}$ approximates the derivative $f'(x)$ up to an $O(\Delta x^2)$ error.
To numerically verify this, we design the following experiment using a log of logistic function (i.e. $f(x) = \log\{ \exp(x) / (1 + \exp(x)) \}$) as an example.
We evaluate its derivative using an analytical formula and compare it to the centered difference approximations as $\Delta x$ varies from $2^{-1}$ to $2^{-52}$.
Using the starter code provided below, plot the relative error of the numerical approximation in a log-log scale.
What do you expect the slope of the plotted line to be?

```{r}
#' Approximate derivative via centered-difference approximation
approximate_deriv <- function(func, x, dx) {
  return((func(x+dx) - func(x-dx))/(2*dx))
}

log_logistic_func <- function(x) {
  return(log(exp(x)/(1+exp(x))))
}

log_logistic_deriv <- function(x) {
  return(1/(1+exp(x)))
}

# Calculate and plot the relative errors of finite difference approximations
set.seed(615)
x <- rnorm(1)

log2_dx <- - seq(1, 52)
numerical_deriv <- sapply(
  2^log2_dx, function(dx) approximate_deriv(log_logistic_func, x, dx)
)
analytical_deriv <- log_logistic_deriv(x)
rel_err <- abs(
  (analytical_deriv - numerical_deriv) / analytical_deriv
)

fontsize <- 1.2
out_plot = plot(
  log2_dx, log2(rel_err),
  frame = FALSE, # Remove the ugly box
  col = jhu_color$spiritBlue,
  xlab = "log2 stepsize for finite difference",
  ylab = "log2 rel errors",
  xlim = rev(range(log2_dx)),
  cex.lab = fontsize,
  cex.axis = fontsize,
  type = 'l')
```

**Questions:**
Does the relative error plot agree with what the mathematical theory predicts? Explain the observed phenomenon.

Also, what is the slope in the "latter part" of your plotted line, roughly speaking? 
Explain why we observe such a slope in the relative error. 
(Hint: Consider a machine with 10 decimal digits of accuracy. What happens if we numerically differentiate $f(x) = x$ at $x = 1$ with $\Delta x = 0.1, 0.01, 0.001, \ldots$.)

### Analysis of Relative Error Plot
According to the mathematical theory, the error in the numerical approximation from the centered difference is restricted only to the error from truncating the Taylor Series, which is order $O(\Delta x^2)$ and should thus approximately reduce quadratically as the step size is reduced linearly. Based solely on the mathematical theory then, one expects to see the relative errors reduce asymptotically to zero as the step size decreases, more specifically forming a line with slope -2, in the given plot format. However, this is not what is observed in the figure produced above. While there is the expected decay in relative error down to a certain step size, which manifests as linear reduction on the log2 scale of slope -2, the relative error actually increases beyond that step size. This is due to the increasing influence of limited numerical precision at these step sizes. For the larger step sizes, the inherent numerical error in forming the machine representations of quantities involving $\Delta x$ is quite small relative to its value, but this is not the case in the later regime. At those smaller step-sizes, the ratio between machine precision (.Machine\$double.eps = 2^-52) and the step-size itself becomes non-trivial. In these ranges then, there is significant error potential in the calculation of the quantities $2\Delta x, f(x+\Delta x),$ and $f(x-\Delta x)$, thus causing a higher final error in the actual value of the centered difference approximation $\frac{f(x + \Delta x) - f(x - \Delta x)}{2\Delta x}$. For this particular case, the $\Delta x$ values are chosen to be $2^{-z}$ for $z \in \{1,2,\ldots, 52\}$, all of which can be represented exactly using doubles. This means that $2\Delta x$ will be $2^{-z}$ for $z \in \{0,2,\ldots, 51\}$, again all values which can be exactly represented. So, where this machine error issue really causes increase in the absolute error, and thus the relative error, is the numerator. Depending upon the decimal representation of $x$, one can encounter significant error when representing the quantities $x \pm \Delta x$ as floats, which is then compounded by application of $f$. The culmination of these errors should be on the scale of .Machine\$double.eps = 2.22e-16, and will always be divided by the $2\Delta x$ term, so it will be on the order of $\Delta x^{-1}$. Given this fact, as well as the fact that the analytical derivative is fixed for the calculation of the relative error, the corresponding slope on the above log-log plot of relative error should be approximately 1. This error is dominated by the truncation error in the smaller step-size regime, causing the reduction seen above over that range of values. So, the slope in the latter part of the plotted line, which is approximately 1 with noise due to the value of $x$ causing different information losses due to machine precision, should indeed have this value 1 according to this error analysis. 

## Part B: Extention to multivariate functions

As mentioned in the lecture, the finite difference method extends to a function with multivariate input $f: \mathbb{R}^d \to \mathbb{R}$ and can be used to approximate the gradient $\nabla f(\boldsymbol{x})$.
The extension relies on the fact $f(\boldsymbol{x} + \Delta x \, \boldsymbol{e}_i) = f(\boldsymbol{x}) + \Delta x \, \partial_i f(\boldsymbol{x}) + \ldots$ for $i = 1, \ldots, d$.

Using the starter code provided below, implement a function to approximate $\nabla f(\boldsymbol{x})$ via the centered difference method.
To test this function, take a quadratic function $f(x) = - \boldsymbol{x}^\intercal \boldsymbol{\Phi} \boldsymbol{x} / 2$ with positive definite $\boldsymbol{\Phi}$ as an example.
First derive an analytical expression for its gradient.
Then compare a numerically approximated gradient to one computed via an analytical formula.

(In deriving an analytical formula for the gradient, you might find these hints helpful: 
1) any positive definite $\boldsymbol{\Phi}$ admits a "square-root" decomposition $\boldsymbol{\Phi} = \boldsymbol{A}^\intercal \boldsymbol{A}$ and hence $\boldsymbol{x}^\intercal \boldsymbol{\Phi} \boldsymbol{x} = \| \boldsymbol{A} \boldsymbol{x} \|^2$; and
2) by the chain rule, the gradient of $g(\boldsymbol{y})$ for $\boldsymbol{y} = \boldsymbol{A} \boldsymbol{x}$ with respect to $\boldsymbol{x}$ is $\nabla_{\boldsymbol{x}} \left( g \circ \boldsymbol{A} \right) (\boldsymbol{x}) = \boldsymbol{A}^\intercal \nabla_{\boldsymbol{y}} g(\boldsymbol{y})$.)

```{r}
approx_grad <- function(func, x, dx = .Machine$double.eps^(1/3)) {
  dimension = length(x)
  numerical_grad <- rep(0, dimension)
  for(i in 1:dimension){
    e_i = replace(rep(0, dimension), i, 1)
    numerical_grad[i] = (func(x + dx*e_i) - func(x - dx*e_i))/(2*dx)
  }
  return(numerical_grad)
}
set.seed(410)
n_param <- 4
X <- matrix(rnorm(2 * n_param^2), nrow = 2 * n_param, ncol = n_param)
Sigma_inv <- t(X) %*% X
  
#' Calculate log-density of centered Gaussian up to an additive factor
gaussian_logp <- function(x, Sigma_inv) {
  logp <- - .5 * t(x) %*% Sigma_inv %*% x
  return(logp)
}

gaussian_grad <- function(x, Sigma_inv) {
 grad <- -1 * Sigma_inv %*% x
  return(grad)
}

x <- c(3, 1, 4, 1)

analytical_grad <- gaussian_grad(x, Sigma_inv)
numerical_grad <- approx_grad(function(x) gaussian_logp(x, Sigma_inv), x)
testthat::expect_true(are_all_close(
  analytical_grad, numerical_grad, abs_tol = Inf, rel_tol = 1e-3
))
```


## Things to commit/submit
For Part A, completed code and answers to the questions.
For Part B, completed code that passes the test.


# Exercise 3: Continuing development of `hiperglm` &mdash; finding MLE via `stats::optim()` and testing it

We will continue the development of the `hiperglm` package. 

As discussed, our plan is to first implement an MLE finder via a pre-packaged general-purpose BFGS optimizer, against which we can then use to benchmark our custom implementation of iteratively reweighted least squares. 
The idea is that using the pre-packaged function is less error-prone, especially since the gradient function is easy to unit-test.

To be extra safe, we are going to test the MLE finder via BFGS against an even simpler algorithm: the least square for a linear model via the analytical expression $\hat{\boldsymbol{\beta}}_\textrm{mle} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \boldsymbol{X}^\intercal \boldsymbol{y}$.
Complete the following steps toward this goal.
Remember that you can use [`devtools::load_all()`](https://r-pkgs.org/code.html#sec-code-load-all) to help your [package development process](https://r-pkgs.org/code.html#constant-health-checks). 

0. Make and check out (`git branch` and `git checkout`) a development branch (call it `dev` or `develop`) where you will commit the work below.
1. Set up the use of `testthat`.
2. Copy the helper functions `are_all_close()` and `simulate_data()` from the instructor's repo.
3. Test the `are_all_close()` function. 
Write a test to cover at least three cases: the function 1) correctly returns `TRUE`, 2) correctly returns `FALSE` because the relative error is above `rel_tol`, and 3) correctly returns `FALSE` because the absolute error is above `abs_tol`. 
Make sure that the logic of the test is clear and easy to understand &mdash; tests are there to clarify and validate the code's behavior; unreadable tests defeat the purpose.
<br> _Remark:_ This is just an exercise, but in general there is nothing strange about testing a function you use to test another function. It is perfectly reasonable to consider testing any functions complicated enough to potentially contain insiduous bugs.
4. Set up the (currently failing) test to compare the MLE estimated via pseudo-inverse and via BFGS. 
5. Implement the MLE finder via peudo-inverse. 
Please, pretty please, do not calculate an inverse of a matrix. 
(Otherwise, it's fine to use `solve()` instead of `chol()`.)
6. Implement the MLE finder via BFGS:
    a. Implement functions to calculate the log-likelihood and gradient under a linear model. 
    This function technically requires an estimate of the residual variance, but its value does not affect the MLE for the coefficients and hence is irrelevant for our purpose. 
    You can just specify it as an optional parameter e.g. as `function(..., noise_var = 1)`.
    b. Test your gradient calculation by comparing it against a numerical one.
    For this purpose, use the function from Exercise 2, Part B to approximate the gradient of a given function via finite difference.
    <br> _Remark:_ Technically, this test checks only for consistency between the log-likelihood and gradient function.
    It is possible to have both functions wrong but consistent with each other so that the test passes.
    This test nonetheless can catch many bugs and, since the gradient calculation tends to be more error prone, it's good to make sure there is no mistake there. 
    c. Plug in the now tested-and-true log-likelihood and gradient functions to `stats::optim()` to find MLE.
7. Run `devtools::test()` and check that the then-failing test from Step 2 now passes.
8. Open a pull request from your development branch to the main/master one. 
    
## Things to commit/submit

Link to the pull request.

The pull request can be found [here](https://github.com/JSartini/hiperglm/pull/1)
